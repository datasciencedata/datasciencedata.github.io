---

title: Python NetworkX Module
featured-img: code
mathjax: false
categories: [python, programming]
description: NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. It has powerful data structures for graphs, digraphs and multigraph and so on.
---

### What is it that neural networks actually learn?

![](videos/what_nns_learn/hyperbolic_tangent_3-layer_2-input_1-output_plots.gif)

Neural networks are famously difficult to interpret. It’s hard to know what they are actually learning when we train them. Let’s take a closer look and see whether we can build a good picture of what’s going on inside.

Just like every other supervised machine learning model, neural networks learn relationships between input variables and output variables. In fact, we can even see how it’s related to the most iconic model of all, linear regression.

### Linear regression

Linear regression assumes a straight line relationship between an input variable _x_ and an output variable _y_. _x_ is multiplied by a constant, _m_, which also happens to be the slope of the line, and it’s added to another constant, _b_, which happens to be where the line crosses the _y_ axis.

![](/assets/images/what_nns_learn/eqn_100.png)

We can represent this in a picture. Our input value _x_ is multiplied by _m_. Our constant _b_, is multiplied by one. And then they are added together to get _y_. This is a graphical representation of _y_ equals _mx_ plus _b_.

![](/assets/images/what_nns_learn/drawing_100.png)

On the far left the circular symbols just indicate that the value is passed through. The rectangles labeled _m_ and _b_ indicate that whatever goes in on the left comes out multiplied By _m_ or _b_ on the right. And the box with the capital sigma indicates that whatever goes in on the left gets added together and spit back out on the right.

We can change the names of all the symbols for a different representation.

![](/assets/images/what_nns_learn/eqn_110.png) 


![](/assets/images/what_nns_learn/drawing_110.png)

This is still a straight line relationship. We have just changed the names of all the variables. The reason we're doing this is to translate our linear regression into the notation we use in neural networks. This will help us keep track of things as we move forward.

At this point, we have turned a straight line equation into a network. A network is anything that has nodes connected by edges. In this case _x\_0_ and _x\_1_ are our input nodes, _v\_0_ is an output node, and our weights connecting them are edges. The mathematical term for a construct like this is a graph. This is not the traditional sense of a graph, meaning a plot, or a grid like in a graphing calculator or graph paper. It’s just the formal word for a network, for nodes connected by edges. Another piece of terminology you might hear is _directed acyclic graph_, abbreviated as DAG. A directed graph is one where the edges just go in one direction. In our case, input goes to output, but output never goes back to input. Our edges are directed. Acyclic means that you can’t ever draw a loop. Once you have visited a node, there is no way to jump from edges to nodes to get back to where you started. Everything flows in one direction through the graph.

We can get a sense of the type of models that this network is capable of learning by choosing random values for the weights, _w\_00_ and _w\_10_, then seeing what relationships pop out between _x\_1_ and _v\_0_. Remember that we set _x\_0_ to 1 and are holding it there always. It is a special node called the bias node.

![](/assets/videos/what_nns_learn/linear_1-layer_1-input_1-output_plots.gif)
